{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn-lstm_classification",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOWCGxOvJW82NUeJ67TPwLz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ekaagra08/CNN-LSTM_video_classification/blob/main/cnn_lstm_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f55cQaanuNzR",
        "outputId": "7d43f31d-5f37-4495-f8d0-3c93dcf5da34"
      },
      "source": [
        "!pip install -q imageio\n",
        "!pip install -q opencv-python\n",
        "!pip install -q git+https://github.com/tensorflow/docs"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGu7JNJSxRtv"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from imutils import paths\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import argparse\n",
        "import pickle\n",
        "import cv2\n",
        "import os\n",
        "import math\n",
        "\n",
        "# TensorFlow and TF-Hub modules.\n",
        "from absl import logging\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow_docs.vis import embed\n",
        "\n",
        "logging.set_verbosity(logging.ERROR)\n",
        "\n",
        "# Some modules to help with reading the UCF101 dataset.\n",
        "import random\n",
        "import re\n",
        "import tempfile\n",
        "import ssl\n",
        "\n",
        "# Some modules to display an animation using imageio.\n",
        "import imageio\n",
        "from IPython import display\n",
        "\n",
        "from urllib import request  # requires python3"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaF4WBTX2YHj"
      },
      "source": [
        "# loading UCF101 dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLgjvltPAN-2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eeb1ba56-4101-4aaa-bb05-67e53fda2b85"
      },
      "source": [
        "!wget --no-check-certificate https://www.crcv.ucf.edu/data/UCF101/UCF101.rar\n",
        "!wget -q --no-check-certificate https://www.crcv.ucf.edu/data/UCF101/UCF101TrainTestSplits-RecognitionTask.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-26 16:16:09--  https://www.crcv.ucf.edu/data/UCF101/UCF101.rar\n",
            "Resolving www.crcv.ucf.edu (www.crcv.ucf.edu)... 132.170.214.127\n",
            "Connecting to www.crcv.ucf.edu (www.crcv.ucf.edu)|132.170.214.127|:443... connected.\n",
            "WARNING: cannot verify www.crcv.ucf.edu's certificate, issued by ‘CN=InCommon RSA Server CA,OU=InCommon,O=Internet2,L=Ann Arbor,ST=MI,C=US’:\n",
            "  Unable to locally verify the issuer's authority.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6932971618 (6.5G) [application/rar]\n",
            "Saving to: ‘UCF101.rar’\n",
            "\n",
            "UCF101.rar          100%[===================>]   6.46G  36.1MB/s    in 2m 15s  \n",
            "\n",
            "2021-11-26 16:18:24 (48.9 MB/s) - ‘UCF101.rar’ saved [6932971618/6932971618]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLVT7KwdNyKw"
      },
      "source": [
        "%%capture\n",
        "!unrar x UCF101.rar data/\n",
        "!unzip -qq UCF101TrainTestSplits-RecognitionTask.zip"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Iqq3h2bq06e"
      },
      "source": [
        "#dataset prepration and pre-preprocessingṁ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prXjhFMTrnNy",
        "outputId": "105ca2c5-a181-4452-89ab-4547b59710fd"
      },
      "source": [
        "image_size= 64 # ie. 64*64\n",
        "\n",
        "#number of frames to passs at a time in a sequence\n",
        "sequence_len = 30\n",
        "\n",
        "#data directoy\n",
        "dataset_dir=\"/content/data/UCF-101\"\n",
        "\n",
        "#selected classes\n",
        "calss_names_path = \"/content/ucfTrainTestlist/classInd.txt\"\n",
        "class_list = []\n",
        "with open(calss_names_path,\"r\") as f:\n",
        "  for line in f:\n",
        "    class_list.append(line.split()[1])\n",
        "\n",
        "f.close()\n",
        "\n",
        "print(class_list)\n",
        "\n",
        "#subset list that we wil work on\n",
        "selected_class_list = [\"Swing\",\"TaiChi\",\"HorseRace\"]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam', 'BandMarching', 'BaseballPitch', 'Basketball', 'BasketballDunk', 'BenchPress', 'Biking', 'Billiards', 'BlowDryHair', 'BlowingCandles', 'BodyWeightSquats', 'Bowling', 'BoxingPunchingBag', 'BoxingSpeedBag', 'BreastStroke', 'BrushingTeeth', 'CleanAndJerk', 'CliffDiving', 'CricketBowling', 'CricketShot', 'CuttingInKitchen', 'Diving', 'Drumming', 'Fencing', 'FieldHockeyPenalty', 'FloorGymnastics', 'FrisbeeCatch', 'FrontCrawl', 'GolfSwing', 'Haircut', 'Hammering', 'HammerThrow', 'HandstandPushups', 'HandstandWalking', 'HeadMassage', 'HighJump', 'HorseRace', 'HorseRiding', 'HulaHoop', 'IceDancing', 'JavelinThrow', 'JugglingBalls', 'JumpingJack', 'JumpRope', 'Kayaking', 'Knitting', 'LongJump', 'Lunges', 'MilitaryParade', 'Mixing', 'MoppingFloor', 'Nunchucks', 'ParallelBars', 'PizzaTossing', 'PlayingCello', 'PlayingDaf', 'PlayingDhol', 'PlayingFlute', 'PlayingGuitar', 'PlayingPiano', 'PlayingSitar', 'PlayingTabla', 'PlayingViolin', 'PoleVault', 'PommelHorse', 'PullUps', 'Punch', 'PushUps', 'Rafting', 'RockClimbingIndoor', 'RopeClimbing', 'Rowing', 'SalsaSpin', 'ShavingBeard', 'Shotput', 'SkateBoarding', 'Skiing', 'Skijet', 'SkyDiving', 'SoccerJuggling', 'SoccerPenalty', 'StillRings', 'SumoWrestling', 'Surfing', 'Swing', 'TableTennisShot', 'TaiChi', 'TennisSwing', 'ThrowDiscus', 'TrampolineJumping', 'Typing', 'UnevenBars', 'VolleyballSpiking', 'WalkingWithDog', 'WallPushups', 'WritingOnBoard', 'YoYo']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2II1brZ7Opx"
      },
      "source": [
        "# Helper functions\n",
        "\n",
        "#crops center portioin of frame\n",
        "def crop_center_square(frame):\n",
        "  y, x = frame.shape[0:2]\n",
        "  min_dim = min(y, x)\n",
        "  start_x = (x // 2) - (min_dim // 2)\n",
        "  start_y = (y // 2) - (min_dim // 2)\n",
        "  return frame[start_y:start_y+min_dim,start_x:start_x+min_dim]\n",
        "\n",
        "######################################################################################  \n",
        "\n",
        "# returns normalized numpy array of each frame(image) in video(sequence of images) \n",
        "def extract_frames(path, max_frames=0, resize=(224, 224)):\n",
        "  cap = cv2.VideoCapture(path)\n",
        "  frames_list = []\n",
        "  #frameRate = cap.get(5) #frame rate \n",
        "  video_frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "  skip_frame_window = max(int(video_frame_count / sequence_len),1)\n",
        "  \n",
        "  try:\n",
        "    for frame_counter in range(sequence_len):\n",
        "      cap.set(cv2.CAP_PROP_POS_FRAMES, frame_counter*skip_frame_window) #current frame number\n",
        "      \n",
        "      ret, frame = cap.read()\n",
        "      \n",
        "      if not ret:\n",
        "        break\n",
        "\n",
        "      #if (frameId % math.floor(frameRate) == 0):\n",
        "      croped_frame = crop_center_square(frame) #crop frame\n",
        "      resized_frame = cv2.resize(croped_frame, resize) #resize frame\n",
        "      frame = frame[:, :, [2, 1, 0]]\n",
        "      frames_list.append(resized_frame)\n",
        "\n",
        "      if len(frames_list ) == max_frames:\n",
        "        break\n",
        "  finally:\n",
        "    cap.release()\n",
        "  print(\"[INFO] {} Frames extracted\".format(len(frames_list)))\n",
        "  # return normalized frames list \n",
        "  return np.array(frames_list ) / 255.0\n",
        "\n",
        "######################################################################################  \n",
        "\n",
        "\n",
        "def to_gif(images):\n",
        "  converted_images = np.clip(images * 255, 0, 255).astype(np.uint8)\n",
        "  imageio.mimsave('./animation.gif', converted_images, fps=25)\n",
        "  return embed.embed_file('./animation.gif')\n",
        "\n",
        "######################################################################################  \n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "zm2dMsURtH9T",
        "outputId": "726da16e-0368-417a-e53f-79db36258770"
      },
      "source": [
        "to_gif(extract_frames(\"/content/data/UCF-101/BaseballPitch/v_BaseballPitch_g01_c03.avi\")) "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] 30 Frames extracted\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-d5621934d32c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mto_gif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/data/UCF-101/BaseballPitch/v_BaseballPitch_g01_c03.avi\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-4cb0cf59e2a5>\u001b[0m in \u001b[0;36mto_gif\u001b[0;34m(images)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mto_gif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0mconverted_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m   \u001b[0mimageio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmimsave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./animation.gif'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconverted_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./animation.gif'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imageio/core/functions.py\u001b[0m in \u001b[0;36mmimwrite\u001b[0;34m(uri, ims, format, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0;31m# Add image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m             \u001b[0mwritten\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imageio/core/format.py\u001b[0m in \u001b[0;36mappend_data\u001b[0;34m(self, im, meta)\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m             \u001b[0;31m# Call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_append_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mset_meta_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imageio/plugins/pillowmulti.py\u001b[0m in \u001b[0;36m_append_data\u001b[0;34m(self, im, meta)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mdispose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdispose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imageio/plugins/pillowmulti.py\u001b[0m in \u001b[0;36madd_image\u001b[0;34m(self, im, duration, dispose)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt_subrectangle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mim_rect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetSubRectangle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mim_pil\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverToPIL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_rect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt_quantizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt_palette_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;31m# Get pallette - apparently, this is the 3d element of the header\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imageio/plugins/pillowmulti.py\u001b[0m in \u001b[0;36mconverToPIL\u001b[0;34m(self, im, quantizer, palette_size)\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m                 \u001b[0mim_pil\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m             \u001b[0mim_pil\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpalette_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquantizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid value for quantizer: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mquantizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mquantize\u001b[0;34m(self, colors, method, kmeans, palette, dither)\u001b[0m\n\u001b[1;32m   1093\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1095\u001b[0;31m         \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImagePalette\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MU7-s6a_Ws9q"
      },
      "source": [
        "#Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgNcs6VA7PXu"
      },
      "source": [
        "def create_dataset():\n",
        "  features=[] #list of multiple lists of frames from one video class (X)\n",
        "  lables=[] #list of class lables (y)\n",
        "  video_file_paths=[]\n",
        "\n",
        "  for class_index, class_name in enumerate(selected_class_list):\n",
        "    print(\"[INFO] Exgtracting data of class: {}\".format(class_name))\n",
        "\n",
        "    #list of all video files in one class directory\n",
        "    files_list = os.listdir(os.path.join(dataset_dir, class_name))\n",
        "\n",
        "    #iterating through all files in class\n",
        "    for file_name in files_list:\n",
        "      #get complete video path\n",
        "      video_file_path = os.path.join(dataset_dir, class_name, file_name)\n",
        "\n",
        "      #extracting frames of the video\n",
        "      frames = extract_frames(video_file_path)\n",
        "      \n",
        "      #select if no. of frames match sequence length else dont select\n",
        "     # if len(frames) == sequence_len:\n",
        "      features.append(frames)\n",
        "      lables.append(class_index)\n",
        "      video_file_paths.append(video_file_path)\n",
        "  return features, lables, video_file_paths         "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-OpkJDRnrNh"
      },
      "source": [
        "features, labels, video_file_paths = create_dataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf5NPKQfy0d2"
      },
      "source": [
        "data = []\n",
        "labels = []\n",
        "#### loop over the image paths\n",
        "for imagePath in imagePaths:\n",
        "    # extract the class label from the filename\n",
        "    label = imagePath.split(os.path.sep)[-2]\n",
        "    # if the label of the current image is not part of of the labels\n",
        "    # are interested in, then ignore the image\n",
        "    if label not in LABELS:\n",
        "        continue\n",
        "    # load the image, convert it to RGB channel ordering, and resize\n",
        "    # it to be a fixed 224x224 pixels, ignoring aspect ratio\n",
        "    image = cv2.imread(imagePath)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image = cv2.resize(image, (224, 224))\n",
        "    # update the data and labels lists, respectively\n",
        "    data.append(image)\n",
        "    labels.append(label)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUvSDEuwCoCc"
      },
      "source": [
        "len(features[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmG9Q_AKy7MF"
      },
      "source": [
        "# convert the data and labels to NumPy arrays\n",
        "features = np.array(features)\n",
        "labels = np.array(labels)\n",
        "# perform one-hot encoding on the labels\n",
        "one_hot_encoded_labels = to_categorical(labels)\n",
        "#lb = LabelBinarizer()\n",
        "#labels = lb.fit_transform(labels)\n",
        "# partition the data into training and testing splits using 75% of\n",
        "# the data for training and the remaining 25% for testing\n",
        "(trainX, testX, trainY, testY) = train_test_split(features, labels,\n",
        "\ttest_size=0.25,shuffle=True, stratify=labels, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlTZ32rZMLmn"
      },
      "source": [
        "def create_cnn-lstm_model():\n",
        "  model=Sequential()\n",
        "\n",
        "  model.add(ConvLSTM2D(filters=4, kernel_size=(3,3), activation='tanh', data_format='channels_last', \n",
        "                       recurrent_dropout=0.2, return_sequences=True, input_shape = (sequence_len,image_size,image_size,3)))\n",
        "  model.add(MaxPooling3D(pool_size=(1,2,2), padding='same', data_format='channels_last'))\n",
        "  model.add(TimeDistributed(Dropout(.02)))\n",
        "  \n",
        "\n",
        "  model.add(ConvLSTM2D(filters=8, kernel_size=(3,3), activation='tanh', data_format='channels_last', \n",
        "                       recurrent_dropout=0.2, return_sequences=True))\n",
        "  model.add(MaxPooling3D(pool_size=(1,2,2), padding='same', data_format='channels_last'))\n",
        "  model.add(TimeDistributed(Dropout(.02)))\n",
        "  \n",
        "  \n",
        "  model.add(ConvLSTM2D(filters=14, kernel_size=(3,3), activation='tanh', data_format='channels_last', \n",
        "                       recurrent_dropout=0.2, return_sequences=True))\n",
        "  model.add(MaxPooling3D(pool_size=(1,2,2), padding='same', data_format='channels_last'))\n",
        "  model.add(TimeDistributed(Dropout(.02)))\n",
        "  \n",
        "  \n",
        "  model.add(ConvLSTM2D(filters=16, kernel_size=(3,3), activation='tanh', data_format='channels_last', \n",
        "                       recurrent_dropout=0.2, return_sequences=True))\n",
        "  model.add(MaxPooling3D(pool_size=(1,2,2), padding='same', data_format='channels_last'))\n",
        "  #model.add(TimeDistributed(Dropout(.02)))\n",
        "\n",
        "  model.add(Flatten())\n",
        "\n",
        "  model.add(Dense(len(selected_class_list), activation='softmax'))\n",
        "\n",
        "  model.summary\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0_imrh5MMtG"
      },
      "source": [
        "convlstm_model= create_cnn-lstm_model()\n",
        "print(\"model created\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCVMg6FgMMYW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxCbVOdIMMIl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG1I5nSp8M5C"
      },
      "source": [
        "# initialize the training data augmentation object\n",
        "trainAug = ImageDataGenerator(\n",
        "\trotation_range=30,\n",
        "\tzoom_range=0.15,\n",
        "\twidth_shift_range=0.2,\n",
        "\theight_shift_range=0.2,\n",
        "\tshear_range=0.15,\n",
        "\thorizontal_flip=True,\n",
        "\tfill_mode=\"nearest\")\n",
        "# initialize the validation/testing data augmentation object (which\n",
        "# we'll be adding mean subtraction to)\n",
        "valAug = ImageDataGenerator()\n",
        "# define the ImageNet mean subtraction (in RGB order) and set the\n",
        "# the mean subtraction value for each of the data augmentation\n",
        "# objects\n",
        "mean = np.array([123.68, 116.779, 103.939], dtype=\"float32\")\n",
        "trainAug.mean = mean\n",
        "valAug.mean = mean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eq8FxOID8Peh"
      },
      "source": [
        "# load the ResNet-50 network, ensuring the head FC layer sets are left\n",
        "# off\n",
        "baseModel = ResNet50(weights=\"imagenet\", include_top=False,\n",
        "\tinput_tensor=Input(shape=(224, 224, 3)))\n",
        "# construct the head of the model that will be placed on top of the\n",
        "# the base model\n",
        "headModel = baseModel.output\n",
        "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
        "headModel = Flatten(name=\"flatten\")(headModel)\n",
        "headModel = Dense(512, activation=\"relu\")(headModel)\n",
        "headModel = Dropout(0.5)(headModel)\n",
        "headModel = Dense(len(lb.classes_), activation=\"softmax\")(headModel)\n",
        "# place the head FC model on top of the base model (this will become\n",
        "# the actual model we will train)\n",
        "model = Model(inputs=baseModel.input, outputs=headModel)\n",
        "# loop over all layers in the base model and freeze them so they will\n",
        "# *not* be updated during the training process\n",
        "for layer in baseModel.layers:\n",
        "\tlayer.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26e2hmOK8R99"
      },
      "source": [
        "# compile our model (this needs to be done after our setting our\n",
        "# layers to being non-trainable)\n",
        "print(\"[INFO] compiling model...\")\n",
        "opt = SGD(lr=1e-4, momentum=0.9, decay=1e-4 / args[\"epochs\"])\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
        "\tmetrics=[\"accuracy\"])\n",
        "# train the head of the network for a few epochs (all other layers\n",
        "# are frozen) -- this will allow the new FC layers to start to become\n",
        "# initialized with actual \"learned\" values versus pure random\n",
        "print(\"[INFO] training head...\")\n",
        "H = model.fit(\n",
        "\tx=trainAug.flow(trainX, trainY, batch_size=32),\n",
        "\tsteps_per_epoch=len(trainX) // 32,\n",
        "\tvalidation_data=valAug.flow(testX, testY),\n",
        "\tvalidation_steps=len(testX) // 32,\n",
        "\tepochs=args[\"epochs\"])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "Ux0ydy-N8UCd",
        "outputId": "f59b1629-2ab3-485b-d961-ed86377fdede"
      },
      "source": [
        "# evaluate the network\n",
        "print(\"[INFO] evaluating network...\")\n",
        "predictions = model.predict(x=testX.astype(\"float32\"), batch_size=32)\n",
        "print(classification_report(testY.argmax(axis=1),\n",
        "\tpredictions.argmax(axis=1), target_names=lb.classes_))\n",
        "# plot the training loss and accuracy\n",
        "N = args[\"epochs\"]\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.savefig(args[\"plot\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] evaluating network...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-896cdb81a2f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# evaluate the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] evaluating network...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m print(classification_report(testY.argmax(axis=1),\n\u001b[1;32m      5\u001b[0m \tpredictions.argmax(axis=1), target_names=lb.classes_))\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "qtfKjUkU8eUU",
        "outputId": "48298506-442b-450b-be5b-8205a2e50fb3"
      },
      "source": [
        "# serialize the model to disk\n",
        "print(\"[INFO] serializing network...\")\n",
        "model.save(args[\"model\"], save_format=\"h5\")\n",
        "# serialize the label binarizer to disk\n",
        "f = open(args[\"label_bin\"], \"wb\")\n",
        "f.write(pickle.dumps(lb))\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] serializing network...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-986451e02c68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# serialize the model to disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] serializing network...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# serialize the label binarizer to disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label_bin\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzKmnDpy8epm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}