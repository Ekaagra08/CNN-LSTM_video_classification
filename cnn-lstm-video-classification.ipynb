{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"!pip install -q imageio\n!pip install -q opencv-python\n!pip install -q git+https://github.com/tensorflow/docs","metadata":{"execution":{"iopub.status.busy":"2021-11-27T14:37:06.879677Z","iopub.execute_input":"2021-11-27T14:37:06.879932Z","iopub.status.idle":"2021-11-27T14:37:40.919704Z","shell.execute_reply.started":"2021-11-27T14:37:06.879903Z","shell.execute_reply":"2021-11-27T14:37:40.918683Z"}}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers import SGD\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n#from imutils import paths\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport argparse\nimport pickle\nimport cv2\nimport os\nimport math\n\n# TensorFlow and TF-Hub modules.\n#from absl import logging\n#import tensorflow_hub as hub\n#from tensorflow_docs.vis import embed\n\n#logging.set_verbosity(logging.ERROR)\n\n# Some modules to help with reading the UCF101 dataset.\nimport random\nimport re\nimport tempfile\nimport ssl\n\n# Some modules to display an animation using imageio.\n#import imageio\n#from IPython import display\n\n#from urllib import request  # requires python3","metadata":{"execution":{"iopub.status.busy":"2021-11-27T14:59:33.156316Z","iopub.execute_input":"2021-11-27T14:59:33.156906Z","iopub.status.idle":"2021-11-27T14:59:33.166360Z","shell.execute_reply.started":"2021-11-27T14:59:33.156861Z","shell.execute_reply":"2021-11-27T14:59:33.165351Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"image_size= 64 # ie. 64*64\n\n#number of frames to passs at a time in a sequence\nsequence_len = 10\n\n#data directoy\ndataset_dir=\"../input/ucf101/UCF101/UCF-101\"\n\n#selected classes\ncalss_names_path = \"../input/ucf101/UCF101TrainTestSplits-RecognitionTask/ucfTrainTestlist/classInd.txt\"#\n\n# class_list stores all the 101 classes\n#class_list = []\n#with open(calss_names_path,\"r\") as f:\n#  for line in f:\n#    class_list.append(line.split()[1])\n#\n#f.close()\n\n#print(class_list)\n\n#subset list that we wil work on\nselected_class_list = [\"TaiChi\",\"HorseRace\"]","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:14:13.450870Z","iopub.execute_input":"2021-11-27T15:14:13.451602Z","iopub.status.idle":"2021-11-27T15:14:13.456354Z","shell.execute_reply.started":"2021-11-27T15:14:13.451558Z","shell.execute_reply":"2021-11-27T15:14:13.455625Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# Helper functions\n\n#crops center portioin of frame\ndef crop_center_square(frame):\n  y, x = frame.shape[0:2]\n  min_dim = min(y, x)\n  start_x = (x // 2) - (min_dim // 2)\n  start_y = (y // 2) - (min_dim // 2)\n  return frame[start_y:start_y+min_dim,start_x:start_x+min_dim]\n\n######################################################################################  \n\n# returns normalized numpy array of each frame(image) in video(sequence of images) \ndef extract_frames(path, max_frames=0, resize=(224, 224)):\n  cap = cv2.VideoCapture(path)\n  frames_list = []\n  #frameRate = cap.get(5) #frame rate \n  video_frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n  skip_frame_window = max(int(video_frame_count / sequence_len),1)\n  \n  try:\n    for frame_counter in range(sequence_len):\n      cap.set(cv2.CAP_PROP_POS_FRAMES, frame_counter*skip_frame_window) #current frame number\n      \n      ret, frame = cap.read()\n      \n      if not ret:\n        break\n\n      #if (frameId % math.floor(frameRate) == 0):\n      croped_frame = crop_center_square(frame) #crop frame\n      resized_frame = cv2.resize(croped_frame, resize) #resize frame\n      frame = frame[:, :, [2, 1, 0]]\n      frames_list.append(resized_frame)\n\n      if len(frames_list ) == max_frames:\n        break\n  finally:\n    cap.release()\n  #print(\"[INFO] {} Frames extracted\".format(len(frames_list)))\n  # return normalized frames list \n  return np.array(frames_list ) / 255.0\n\n######################################################################################  \n\n\n#def to_gif(images):\n  #converted_images = np.clip(images * 255, 0, 255).astype(np.uint8)\n  #imageio.mimsave('./animation.gif', converted_images, fps=25)\n  #return embed.embed_file('./animation.gif')\n\n######################################################################################  \n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-27T14:59:36.635228Z","iopub.execute_input":"2021-11-27T14:59:36.635528Z","iopub.status.idle":"2021-11-27T14:59:36.647795Z","shell.execute_reply.started":"2021-11-27T14:59:36.635493Z","shell.execute_reply":"2021-11-27T14:59:36.646524Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Create Dataset","metadata":{"execution":{"iopub.status.busy":"2021-11-27T14:26:35.590613Z","iopub.execute_input":"2021-11-27T14:26:35.590935Z","iopub.status.idle":"2021-11-27T14:26:35.595187Z","shell.execute_reply.started":"2021-11-27T14:26:35.590899Z","shell.execute_reply":"2021-11-27T14:26:35.594004Z"}}},{"cell_type":"code","source":"def create_dataset():\n  features=[] #list of multiple lists of frames from one video class (X)\n  lables=[] #list of class lables (y)\n  video_file_paths=[]\n\n  for class_index, class_name in enumerate(selected_class_list):\n    print(\"[INFO] Exgtracting data of class: {}\".format(class_name))\n\n    #list of all video files in one class directory\n    files_list = os.listdir(os.path.join(dataset_dir, class_name))\n\n    #iterating through all files in class\n    for file_name in files_list:\n      #get complete video path\n      video_file_path = os.path.join(dataset_dir, class_name, file_name)\n\n      #extracting frames of the video\n      frames = extract_frames(video_file_path,resize=(image_size,image_size))\n      \n      #select if no. of frames match sequence length else dont select\n     # if len(frames) == sequence_len:\n      features.append(frames)\n      lables.append(class_index)\n      video_file_paths.append(video_file_path)\n  return features, lables, video_file_paths         ","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:14:36.944134Z","iopub.execute_input":"2021-11-27T15:14:36.944416Z","iopub.status.idle":"2021-11-27T15:14:36.952965Z","shell.execute_reply.started":"2021-11-27T15:14:36.944387Z","shell.execute_reply":"2021-11-27T15:14:36.951840Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"features, labels, video_file_paths = create_dataset()","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:14:39.791631Z","iopub.execute_input":"2021-11-27T15:14:39.791954Z","iopub.status.idle":"2021-11-27T15:14:52.391329Z","shell.execute_reply.started":"2021-11-27T15:14:39.791922Z","shell.execute_reply":"2021-11-27T15:14:52.390348Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"[INFO] Exgtracting data of class: TaiChi\n[INFO] Exgtracting data of class: HorseRace\n","output_type":"stream"}]},{"cell_type":"code","source":"# convert the data and labels to NumPy arrays\nfeatures = np.array(features)\nlabels = np.array(labels)\n# perform one-hot encoding on the labels\none_hot_encoded_labels = to_categorical(labels)\n#lb = LabelBinarizer()\n#labels = lb.fit_transform(labels)\n# partition the data into training and testing splits using 75% of\n# the data for training and the remaining 25% for testing\n(trainX, testX, trainY, testY) = train_test_split(features, labels,\n\ttest_size=0.25,shuffle=True, stratify=labels, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:14:54.292539Z","iopub.execute_input":"2021-11-27T15:14:54.292832Z","iopub.status.idle":"2021-11-27T15:14:54.458588Z","shell.execute_reply.started":"2021-11-27T15:14:54.292802Z","shell.execute_reply":"2021-11-27T15:14:54.457513Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"trainX.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:15:00.771780Z","iopub.execute_input":"2021-11-27T15:15:00.772128Z","iopub.status.idle":"2021-11-27T15:15:00.778746Z","shell.execute_reply.started":"2021-11-27T15:15:00.772082Z","shell.execute_reply":"2021-11-27T15:15:00.777660Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"(168, 10, 64, 64, 3)"},"metadata":{}}]},{"cell_type":"code","source":"def create_cnnlstm_model():\n  model=Sequential()\n\n  model.add(ConvLSTM2D(filters=4, kernel_size=(3,3), activation='tanh', data_format='channels_last', \n                       recurrent_dropout=0.2, return_sequences=True, input_shape = (sequence_len,image_size,image_size,3)))\n  model.add(MaxPooling3D(pool_size=(1,2,2), padding='same', data_format='channels_last'))\n  model.add(TimeDistributed(Dropout(0.2)))\n  \n\n  model.add(ConvLSTM2D(filters=8, kernel_size=(3,3), activation='tanh', data_format='channels_last', \n                       recurrent_dropout=0.2, return_sequences=True))\n  model.add(MaxPooling3D(pool_size=(1,2,2), padding='same', data_format='channels_last'))\n  model.add(TimeDistributed(Dropout(0.2)))\n  \n  \n  model.add(ConvLSTM2D(filters=14, kernel_size=(3,3), activation='tanh', data_format='channels_last', \n                       recurrent_dropout=0.2, return_sequences=True))\n  model.add(MaxPooling3D(pool_size=(1,2,2), padding='same', data_format='channels_last'))\n  model.add(TimeDistributed(Dropout(0.2)))\n  \n  \n  model.add(ConvLSTM2D(filters=16, kernel_size=(3,3), activation='tanh', data_format='channels_last', \n                       recurrent_dropout=0.2, return_sequences=True))\n  model.add(MaxPooling3D(pool_size=(1,2,2), padding='same', data_format='channels_last'))\n  #model.add(TimeDistributed(Dropout(0.2)))\n\n  model.add(Flatten())\n\n  model.add(Dense(len(selected_class_list), activation='softmax'))\n\n  model.summary\n\n  return model","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:20:37.205011Z","iopub.execute_input":"2021-11-27T15:20:37.205835Z","iopub.status.idle":"2021-11-27T15:20:37.218883Z","shell.execute_reply.started":"2021-11-27T15:20:37.205789Z","shell.execute_reply":"2021-11-27T15:20:37.218110Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"convlstm_model= create_cnnlstm_model()\nprint(\"model created\")","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:20:37.993354Z","iopub.execute_input":"2021-11-27T15:20:37.993878Z","iopub.status.idle":"2021-11-27T15:20:38.503836Z","shell.execute_reply.started":"2021-11-27T15:20:37.993843Z","shell.execute_reply":"2021-11-27T15:20:38.503207Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"model created\n","output_type":"stream"}]},{"cell_type":"code","source":"epochs=30\nbatch_size=5","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:20:38.505232Z","iopub.execute_input":"2021-11-27T15:20:38.505839Z","iopub.status.idle":"2021-11-27T15:20:38.509313Z","shell.execute_reply.started":"2021-11-27T15:20:38.505809Z","shell.execute_reply":"2021-11-27T15:20:38.508735Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10, mode='min', restore_best_weights=True )\nconvlstm_model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n\nmodel_training = convlstm_model.fit(x=trainX, y=trainY, epochs=epochs, batch_size=batch_size,\n                                    shuffle=True, validation_split=0.2, callbacks=[early_stopping_callback])","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:26:08.151805Z","iopub.execute_input":"2021-11-27T15:26:08.152605Z","iopub.status.idle":"2021-11-27T15:32:29.970081Z","shell.execute_reply.started":"2021-11-27T15:26:08.152551Z","shell.execute_reply":"2021-11-27T15:32:29.968994Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"Epoch 1/30\n27/27 [==============================] - 32s 730ms/step - loss: 0.6975 - accuracy: 0.4733 - val_loss: 0.6896 - val_accuracy: 0.6471\nEpoch 2/30\n27/27 [==============================] - 18s 677ms/step - loss: 0.6889 - accuracy: 0.6292 - val_loss: 0.6571 - val_accuracy: 0.5588\nEpoch 3/30\n27/27 [==============================] - 18s 684ms/step - loss: 0.5699 - accuracy: 0.7290 - val_loss: 0.5063 - val_accuracy: 0.7353\nEpoch 4/30\n27/27 [==============================] - 18s 675ms/step - loss: 0.4281 - accuracy: 0.8539 - val_loss: 0.4372 - val_accuracy: 0.7941\nEpoch 5/30\n27/27 [==============================] - 19s 689ms/step - loss: 0.4169 - accuracy: 0.8012 - val_loss: 0.7446 - val_accuracy: 0.6471\nEpoch 6/30\n27/27 [==============================] - 18s 682ms/step - loss: 0.4397 - accuracy: 0.8576 - val_loss: 0.5514 - val_accuracy: 0.6471\nEpoch 7/30\n27/27 [==============================] - 18s 685ms/step - loss: 0.5037 - accuracy: 0.7887 - val_loss: 0.7178 - val_accuracy: 0.7353\nEpoch 8/30\n27/27 [==============================] - 18s 678ms/step - loss: 0.5721 - accuracy: 0.7855 - val_loss: 0.3032 - val_accuracy: 0.8824\nEpoch 9/30\n27/27 [==============================] - 19s 688ms/step - loss: 0.2731 - accuracy: 0.8445 - val_loss: 0.2350 - val_accuracy: 0.9118\nEpoch 10/30\n27/27 [==============================] - 18s 674ms/step - loss: 0.1605 - accuracy: 0.9502 - val_loss: 0.2147 - val_accuracy: 0.9412\nEpoch 11/30\n27/27 [==============================] - 19s 692ms/step - loss: 0.1643 - accuracy: 0.9170 - val_loss: 0.2464 - val_accuracy: 0.9706\nEpoch 12/30\n27/27 [==============================] - 18s 676ms/step - loss: 0.0879 - accuracy: 0.9640 - val_loss: 0.5175 - val_accuracy: 0.8824\nEpoch 13/30\n27/27 [==============================] - 18s 683ms/step - loss: 0.0878 - accuracy: 0.9811 - val_loss: 0.3788 - val_accuracy: 0.9412\nEpoch 14/30\n27/27 [==============================] - 18s 684ms/step - loss: 0.1141 - accuracy: 0.9612 - val_loss: 0.2912 - val_accuracy: 0.8529\nEpoch 15/30\n27/27 [==============================] - 18s 685ms/step - loss: 0.1040 - accuracy: 0.9601 - val_loss: 0.2695 - val_accuracy: 0.9118\nEpoch 16/30\n27/27 [==============================] - 18s 684ms/step - loss: 0.0493 - accuracy: 0.9798 - val_loss: 0.3530 - val_accuracy: 0.9118\nEpoch 17/30\n27/27 [==============================] - 18s 676ms/step - loss: 0.0842 - accuracy: 0.9611 - val_loss: 0.2586 - val_accuracy: 0.8824\nEpoch 18/30\n27/27 [==============================] - 18s 683ms/step - loss: 0.1207 - accuracy: 0.9597 - val_loss: 0.5352 - val_accuracy: 0.9118\nEpoch 19/30\n27/27 [==============================] - 18s 680ms/step - loss: 0.0537 - accuracy: 0.9865 - val_loss: 0.6970 - val_accuracy: 0.7941\nEpoch 20/30\n27/27 [==============================] - 19s 691ms/step - loss: 0.0362 - accuracy: 0.9791 - val_loss: 0.4928 - val_accuracy: 0.9118\n","output_type":"stream"}]},{"cell_type":"code","source":"model_evaluation = convlstm_model.evaluate(testX,testY)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:32:29.971957Z","iopub.execute_input":"2021-11-27T15:32:29.972240Z","iopub.status.idle":"2021-11-27T15:32:31.732404Z","shell.execute_reply.started":"2021-11-27T15:32:29.972209Z","shell.execute_reply":"2021-11-27T15:32:31.731620Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"2/2 [==============================] - 2s 723ms/step - loss: 0.2405 - accuracy: 0.8393\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}